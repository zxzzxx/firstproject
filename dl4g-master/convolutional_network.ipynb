{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Networks (ConvNets)\n",
    "======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install pytorch (http://pytorch.org/) if run from Google Colaboratory\n",
    "import sys\n",
    "if 'google.colab' in sys.modules and 'torch' not in sys.modules:\n",
    "    from os.path import exists\n",
    "    from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "    platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "    cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
    "    accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
    "\n",
    "    !pip3 install https://download.pytorch.org/whl/{accelerator}/torch-1.1.0-{platform}-linux_x86_64.whl\n",
    "    !pip3 install https://download.pytorch.org/whl/{accelerator}/torchvision-0.3.0-{platform}-linux_x86_64.whl\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter Settings\n",
    "-------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "use_dropout = False\n",
    "use_gpu = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST Data Loading\n",
    "-------------------\n",
    "\n",
    "MNIST images show digits from 0-9 in 28x28 grayscale images. We normalize and center them around 0, which gives a slight performance boost during training.\n",
    "We create both a training set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78c47c40340e46389fde4716d26542d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data/MNIST\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df6e4c37f4204f1ebdc5d7a92db25f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data/MNIST\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "559c24202fe84ab7975944774284390b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data/MNIST\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef2568b2d3294137a2e7c80deb1bf73f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data/MNIST\\MNIST\\raw\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\torchvision\\datasets\\mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = MNIST(root='./data/MNIST', download=True, train=True, transform=img_transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = MNIST(root='./data/MNIST', download=True, train=False, transform=img_transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConvNet Definition\n",
    "-----------------------\n",
    "Compare the number of parameters to the multi-layer perceptron: the convnet has less than 5% of the MLPs parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Number of parameters: 25384\n"
     ]
    }
   ],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=4, stride=2, padding=1) # out: 8 x 14 x 14\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=4, stride=2, padding=1) # out: 16 x 7 x 7\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=4, stride=2, padding=1) # out: 32 x 3 x 3\n",
    "        if use_dropout:\n",
    "            self.do1 = nn.Dropout2d(p=0.5)\n",
    "        self.fc1 = nn.Linear(288, 50)\n",
    "        if use_dropout:\n",
    "            self.do2 = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(50, 10) # 10 outputs: probability for each digit class\n",
    "\n",
    "    def forward(self, x):\n",
    "        # convolutional part\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        if use_dropout:\n",
    "            x = self.do1(x)\n",
    "        x = x.view(x.size(0), -1) # flatten batch of multi-channel feature maps to a batch of feature vectors\n",
    "        \n",
    "        # fully connected part\n",
    "        x = F.relu(self.fc1(x))\n",
    "        if use_dropout:\n",
    "            x = self.do2(x)\n",
    "        x = F.log_softmax(self.fc2(x), dim=1) # last activation is log softmax to get log class probabilities\n",
    "        \n",
    "        return x\n",
    "\n",
    "convnet = ConvNet()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
    "convnet = convnet.to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in convnet.parameters() if p.requires_grad)\n",
    "print('Number of parameters: %d' % num_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train ConvNet\n",
    "--------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ...\n",
      "Epoch [1 / 20] average loss: 0.445937\n",
      "Epoch [2 / 20] average loss: 0.112218\n",
      "Epoch [3 / 20] average loss: 0.075458\n",
      "Epoch [4 / 20] average loss: 0.057526\n",
      "Epoch [5 / 20] average loss: 0.048210\n",
      "Epoch [6 / 20] average loss: 0.040867\n",
      "Epoch [7 / 20] average loss: 0.035486\n",
      "Epoch [8 / 20] average loss: 0.030914\n",
      "Epoch [9 / 20] average loss: 0.028119\n",
      "Epoch [10 / 20] average loss: 0.023456\n",
      "Epoch [11 / 20] average loss: 0.020504\n",
      "Epoch [12 / 20] average loss: 0.019101\n",
      "Epoch [13 / 20] average loss: 0.016783\n",
      "Epoch [14 / 20] average loss: 0.015474\n",
      "Epoch [15 / 20] average loss: 0.012182\n",
      "Epoch [16 / 20] average loss: 0.013422\n",
      "Epoch [17 / 20] average loss: 0.011305\n",
      "Epoch [18 / 20] average loss: 0.011259\n",
      "Epoch [19 / 20] average loss: 0.009510\n",
      "Epoch [20 / 20] average loss: 0.008359\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(params=convnet.parameters(), lr=learning_rate)\n",
    "\n",
    "# set to training mode\n",
    "convnet.train()\n",
    "\n",
    "train_loss_avg = []\n",
    "\n",
    "print('Training ...')\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss_avg.append(0)\n",
    "    num_batches = 0\n",
    "    \n",
    "    for image_batch, label_batch in train_dataloader:\n",
    "        \n",
    "        image_batch = image_batch.to(device)\n",
    "        label_batch = label_batch.to(device)\n",
    "        \n",
    "        # class predictions\n",
    "        prediction_batch = convnet(image_batch)\n",
    "        \n",
    "        # The cross-entropy loss.\n",
    "        # The first input are the predicted log class probabilities.\n",
    "        # The ground truth probabilites for each image are expected to be\n",
    "        # 1 for a single class and 0 for all other classes.\n",
    "        # This function expects as second input the index of the class with probability 1.\n",
    "        # (this function is not called cross-entropy, since this function assumes\n",
    "        # that the inputs are log probabilities, not probabilities).\n",
    "        loss = F.nll_loss(prediction_batch, label_batch)\n",
    "        \n",
    "        # backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # one step of the optmizer (using the gradients from backpropagation)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss_avg[-1] += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "    train_loss_avg[-1] /= num_batches\n",
    "    print('Epoch [%d / %d] average loss: %f' % (epoch+1, num_epochs, train_loss_avg[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Training Curve\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.ion()\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(train_loss_avg)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Cross-entropy loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively: Load Pre-Trained Model\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'convolutional_network.pth'\n",
    "# filename = 'convolutional_network_dropout.pth'\n",
    "import urllib\n",
    "if not os.path.isdir('./pretrained'):\n",
    "    os.makedirs('./pretrained')\n",
    "print('downloading ...')\n",
    "urllib.request.urlretrieve (\"http://geometry.cs.ucl.ac.uk/creativeai/pretrained/\"+filename, \"./pretrained/\"+filename)\n",
    "convnet.load_state_dict(torch.load('./pretrained/'+filename))\n",
    "print('done')\n",
    "\n",
    "# this is how the model parameters can be saved:\n",
    "# torch.save(convnet.state_dict(), './pretrained/my_convolutional_network.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on the Test Set\n",
    "-------------------------\n",
    "\n",
    "The best current methods achieve a classification error percentage of around 0.21%. See [here](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354) for a leaderboard.\n",
    "\n",
    "Compared to the multi-layer perceptron, the convnet achieves less error using roughly 4% as many parameters.\n",
    "\n",
    "Also notice that there is some overfitting: the average loss is significantly higher than for the training set.\n",
    "Overfitting can be reduced by adding dropout between the fully connected layers. Retrain with `use_dropout` set to `True` or load the pre-trained `convolutional_network_dropout.pth` to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss: 0.048213\n",
      "classification error: 1.250000%\n"
     ]
    }
   ],
   "source": [
    "# set to evaluation mode\n",
    "convnet.eval()\n",
    "\n",
    "num_incorrect = 0\n",
    "test_loss_avg = 0\n",
    "num_batches = 0\n",
    "num_instances = 0\n",
    "for image_batch, label_batch in test_dataloader:\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        image_batch = image_batch.to(device)\n",
    "        label_batch = label_batch.to(device)\n",
    "\n",
    "        # class predictions\n",
    "        prediction_batch = convnet(image_batch)\n",
    "\n",
    "        # get number of correct and incorrect class predictions\n",
    "        _, predicted_label = prediction_batch.max(dim=1)\n",
    "        num_incorrect += (predicted_label != label_batch).sum().item()\n",
    "\n",
    "        # cross-entropy loss\n",
    "        loss = F.nll_loss(prediction_batch, label_batch)\n",
    "\n",
    "        test_loss_avg += loss.item()\n",
    "        num_batches += 1\n",
    "        num_instances += image_batch.size(0)\n",
    "    \n",
    "test_loss_avg /= num_batches\n",
    "print('average loss: %f' % (test_loss_avg))\n",
    "print('classification error: %f%%' % ((num_incorrect / num_instances)*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
